---
{"dg-publish":true,"permalink":"/digital-garden/ai-alignment/","updated":"2023-12-06T16:31:18.000-07:00"}
---

## Inner alignment 
Refers to the alignment between an AI system's goals and the goals that it has been explicitly programmed to pursue. This involves ensuring that the AI system's behavior is consistent with its intended purpose and that it does not deviate from its objectives in unintended ways. Inner alignment is typically achieved through careful design and testing of the AI system's algorithms and programming.

## Outer alignment
Refers to the alignment between an AI system's goals and the goals of its human operators. This involves ensuring that the AI system's behavior is beneficial to humans and aligns with human values and interests, even if those goals are not explicitly programmed into the system. Outer alignment is typically achieved through the use of feedback mechanisms and reward structures that incentivize the AI system to act in ways that are beneficial to humans.

The distinction between inner and outer alignment is important because an AI system can be perfectly aligned with its explicit objectives (i.e., achieve inner alignment) but still act in ways that are harmful or undesirable to humans (i.e., fail to achieve outer alignment>)